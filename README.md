This exercise is meant to understand how a large language model (LLM) generates text--one token at a time, using the previous/initial tokens to predict the next ones.
